{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import BioSimSpace as BSS\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "import itertools as it\n",
    "\n",
    "import pipeline\n",
    "\n",
    "from pipeline.prep import *\n",
    "from pipeline.utils import *\n",
    "\n",
    "pipeline.__file__\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "fwf_path = (\n",
    "    \"/home/anna/Documents/september_2022_workshops/freenrgworkflows/networkanalysis\"\n",
    ")\n",
    "if fwf_path not in sys.path:\n",
    "    sys.path.insert(1, fwf_path)\n",
    "\n",
    "import networkanalysis\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from scipy.stats import sem as sem\n",
    "from scipy.stats import bootstrap, norm\n",
    "from scipy.stats import spearmanr\n",
    "import scipy.stats as _stats\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "protein = \"mcl1\"\n",
    "\n",
    "exec_folder = f\"/home/anna/Documents/benchmark/reruns/{protein}/execution_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:teal\">Comparing lomap and the rbfenn</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the perts in each and all together\n",
    "perts_lomap = []\n",
    "perts_rbfenn = []\n",
    "perts = []\n",
    "\n",
    "file_a = f\"{exec_folder}/network_lomap.dat\"\n",
    "# file_a = f\"a-optimal-lomap.csv\"\n",
    "file_b = f\"{exec_folder}/network_rbfenn.dat\"\n",
    "# file_b = f\"d-optimal-lomap.csv\"\n",
    "\n",
    "perts_rbfenn, ligs = pipeline.utils.get_info_network(file_a)\n",
    "perts_lomap, ligs = pipeline.utils.get_info_network(file_b)\n",
    "perts = perts_rbfenn + perts_lomap\n",
    "\n",
    "# write a file that contains the combined perts, directions are distinct\n",
    "combined_perts = []\n",
    "filtered_out = 0\n",
    "filtered_perts = []\n",
    "for pert in perts:\n",
    "    if not pert in combined_perts:\n",
    "        combined_perts.append(pert)\n",
    "    else:\n",
    "        filtered_out += 1\n",
    "        filtered_perts.append(pert)\n",
    "print(\n",
    "    f\"Removed {filtered_out} duplicate perts between lomap and rbfenn to give {len(combined_perts)} combined perts.\"\n",
    ")\n",
    "\n",
    "# write a file that contains the unique perts, 1 direction only.\n",
    "filtered_perts = []\n",
    "filtered_out = 0\n",
    "for pert in combined_perts:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if not pert in filtered_perts and not inv_pert in filtered_perts:\n",
    "        filtered_perts.append(pert)\n",
    "    else:\n",
    "        filtered_out += 1\n",
    "print(\n",
    "    f\"Removed {filtered_out} inverse perts to give {len(filtered_perts)} unique perts, one direction only.\"\n",
    ")\n",
    "\n",
    "# get the perts that are unique to each\n",
    "unique_perts = []\n",
    "unique_out_lomap = 0\n",
    "unique_out_rbfenn = 0\n",
    "shared_out = 0\n",
    "\n",
    "for pert in perts_lomap:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if not pert in perts_rbfenn and not inv_pert in perts_rbfenn:\n",
    "        unique_perts.append((pert, \"lomap\"))\n",
    "        unique_out_lomap += 1\n",
    "\n",
    "for pert in perts_rbfenn:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if not pert in perts_lomap and not inv_pert in perts_lomap:\n",
    "        unique_perts.append((pert, \"rbfenn\"))\n",
    "        unique_out_rbfenn += 1\n",
    "\n",
    "for pert in combined_perts:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if pert in perts_lomap or inv_pert in perts_lomap:\n",
    "        if pert in perts_rbfenn or inv_pert in perts_rbfenn:\n",
    "            unique_perts.append((pert, \"shared\"))\n",
    "            shared_out += 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"There are {unique_out_lomap} pert(s) unique to lomap and {unique_out_rbfenn} pert(s) unique to rbfenn.\"\n",
    ")\n",
    "print(f\"There are {shared_out} pert(s) shared between lomap and rbfenn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.utils._network import get_info_network\n",
    "\n",
    "prot_dict = {}\n",
    "for prot in [\"tyk2\", \"mcl1\", \"p38\"]:  # , \"cmet\", \"syk\", \"hif2a\"\n",
    "    prot_dict[prot] = {}\n",
    "    for network in [\"lomap\", \"rbfenn\", \"flare\"]:\n",
    "        try:\n",
    "            exec_folder = (\n",
    "                f\"/home/anna/Documents/benchmark/reruns/{prot}/execution_model\"\n",
    "            )\n",
    "            perts, ligs = get_info_network(\n",
    "                net_file=f\"{exec_folder}/network_{network}.dat\",\n",
    "            )\n",
    "            prot_dict[prot][network] = len(perts) / len(ligs)\n",
    "        except:\n",
    "            prot_dict[prot][network] = 0\n",
    "# for prot in [\"cmet\", \"syk\", \"hif2a\"]:\n",
    "#     prot_dict[prot][\"flare\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(prot_dict).T\n",
    "df.plot.bar(\n",
    "    color=[\"darkslateblue\", \"purple\", \"orchid\", \"lavender\"],\n",
    ")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"perturbations/ligands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually combined all perts,\n",
    "perts_all, ligs = pipeline.utils.get_info_network(\"tyk2_all_perts.dat\")\n",
    "print(len(perts_all))\n",
    "# print(perts_all)\n",
    "\n",
    "all_perts = []\n",
    "\n",
    "for pert in perts_all:\n",
    "    inv_pert = pert.split(\"~\")[1] + \"~\" + pert.split(\"~\")[0]\n",
    "\n",
    "    if pert in combined_perts or inv_pert in combined_perts:\n",
    "        all_perts.append((pert, \"old\"))\n",
    "    else:\n",
    "        all_perts.append((pert, \"new\"))\n",
    "\n",
    "df = pd.DataFrame(all_perts)\n",
    "\n",
    "df.loc[df[1] == \"new\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{exec_folder}/combined_perts.dat\", \"w\") as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for pert in combined_perts:\n",
    "        writer.writerow([pert])\n",
    "print(f\"Total number of combined perturbations: {len(combined_perts)}\")\n",
    "\n",
    "# with open(f\"{exec_folder}/filtered_perts.dat\", \"w\") as writefile:\n",
    "#     writer = csv.writer(writefile)\n",
    "#     for pert in filtered_perts:\n",
    "#         writer.writerow([pert])\n",
    "# print(f\"Total number of filtered perturbations: {len(filtered_perts)}\")\n",
    "\n",
    "# # write a file for the different perts\n",
    "# with open(f\"{exec_folder}/unique_perts.dat\", \"w\") as writefile:\n",
    "#     writer = csv.writer(writefile)\n",
    "#     for pert in unique_perts:\n",
    "#         writer.writerow([pert[0], pert[1]])\n",
    "# print(\n",
    "#     f\"Total number of unique perturbations: {len(unique_perts)} (lomap: {unique_out_lomap}, rbfenn: {unique_out_rbfenn})\"\n",
    "# )\n",
    "# print(f\"Total number of shared perturbations: {shared_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a and d optimal networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts are in R, adapted from Yang et al. and run seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = \"p38\"\n",
    "\n",
    "exec_folder = f\"/home/anna/Documents/benchmark/reruns/{protein}/execution_model\"\n",
    "# exec_folder = f\"/home/anna/Documents/benchmark/{protein}_benchmark/execution_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleArray(arr):\n",
    "    \"\"\"Scales an array to be the inverse in the range [0-1].\"\"\"\n",
    "\n",
    "    # normalise to the range 0-1.\n",
    "    return minmax_scale(1 / arr, feature_range=(0.03, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate a network, need to make matrix\n",
    "# RBFENN links file needs to be generated using the RBFENN_setup.ipynb\n",
    "\n",
    "links_file = f\"{exec_folder}/RBFENN/links_file.in\"\n",
    "\n",
    "# rbfenn file\n",
    "\n",
    "value_dict = {}\n",
    "\n",
    "# need to normalise the rbfenn so less close to 0 as otherwise is not able to det(matrix) way more than 0 when using the R script.\n",
    "\n",
    "with open(links_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        lig0 = line.split(\" \")[0].strip()\n",
    "        lig1 = line.split(\" \")[1].strip()\n",
    "        score = line.split(\" \")[2].strip()\n",
    "        if lig0 not in value_dict.keys():\n",
    "            value_dict[lig0] = {}\n",
    "            value_dict[lig0][lig1] = score\n",
    "        else:\n",
    "            value_dict[lig0][lig1] = score\n",
    "\n",
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "df = pd.DataFrame(value_dict).fillna(1)\n",
    "\n",
    "df = df.sort_index()\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "# df.to_csv(\n",
    "#     f\"{exec_folder}/{protein}_rbfenn_matrix.csv\",\n",
    "#     index_label=\"ID\",\n",
    "# )\n",
    "df.to_csv(\n",
    "    f\"/home/anna/Documents/other_workflows/yang2020_optimal_designs/me/{protein}/{protein}_rbfenn_matrix.csv\",\n",
    "    index_label=\"ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lomap file\n",
    "\n",
    "value_dict = {}\n",
    "\n",
    "# initialise pipeline at top of file\n",
    "pl = initialise_pipeline()\n",
    "# where the ligands for the pipeline are located. These should all be in the same folder in sdf format\n",
    "pl.ligands_folder(f\"/home/anna/Documents/benchmark/inputs/reruns/{protein}/ligands\")\n",
    "# where the pipeline should be made\n",
    "pl.main_folder(\"/home/anna/Documents/benchmark/test\")\n",
    "pl.setup_ligands()\n",
    "\n",
    "for lig0, lig1 in it.product(\n",
    "    list(pl.ligands_dict.keys()), list(pl.ligands_dict.keys())\n",
    "):\n",
    "    if lig0 != lig1:\n",
    "        print(lig0, lig1)\n",
    "        single_transformation, score = BSS.Align.generateNetwork(\n",
    "            [pl.ligands_dict[lig0], pl.ligands_dict[lig1]],\n",
    "            names=[lig0, lig1],\n",
    "            plot_network=False,\n",
    "            links_file=None,\n",
    "        )\n",
    "        if lig0 not in value_dict.keys():\n",
    "            value_dict[lig0] = {}\n",
    "            value_dict[lig0][lig1] = score[0]\n",
    "        else:\n",
    "            value_dict[lig0][lig1] = score[0]\n",
    "\n",
    "df = pd.DataFrame(value_dict).fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = scaleArray(df.to_numpy())\n",
    "# df_arr = pd.DataFrame(arr, columns=list(pl.ligands_dict.keys()), index=list(pl.ligands_dict.keys()))\n",
    "df_arr = df\n",
    "\n",
    "# df.to_csv(f\"{exec_folder}/{protein}_lomap_matrix.csv\", index_label=\"ID\")\n",
    "df_arr.to_csv(\n",
    "    f\"/home/anna/Documents/other_workflows/yang2020_optimal_designs/me/{protein}/{protein}_lomap_matrix.csv\",\n",
    "    index_label=\"ID\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline_annamherz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
